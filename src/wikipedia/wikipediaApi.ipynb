{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa724fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_city_info_wikipedia(city_name):\n",
    "    city_information=\"\"\n",
    "    # Passo 1: Primeiro, fazemos uma consulta para obter o ID da página da cidade\n",
    "    url = f'https://pt.wikipedia.org/w/api.php?action=query&format=json&titles={city_name}'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extraindo o ID da página da resposta\n",
    "    pages = data['query']['pages']\n",
    "    page_id = next(iter(pages))  # Pegando o primeiro ID da página (único)\n",
    "    \n",
    "    # Passo 2: Consultar o conteúdo da página usando o ID obtido\n",
    "    if page_id != '-1':  # Se a página existir\n",
    "        city_information += f\"Informações sobre a cidade: {city_name}\\n\"\n",
    "        page_url = f'https://pt.wikipedia.org/wiki/{city_name.replace(\" \", \"_\")}'\n",
    "        city_information += f\"Link para mais detalhes: {page_url}\\n\\n\"\n",
    "        \n",
    "        # Requisição para pegar o conteúdo da página\n",
    "        url_details = f'https://pt.wikipedia.org/w/api.php?action=parse&format=json&pageid={page_id}&prop=text&formatversion=2'\n",
    "        response_details = requests.get(url_details)\n",
    "        data_details = response_details.json()\n",
    "\n",
    "        # Pegando o conteúdo (HTML da página)\n",
    "        content = data_details['parse']['text']\n",
    "        \n",
    "        # Usando BeautifulSoup para parsear o HTML\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Exemplo de busca por informações específicas de Clima, Economia, Educação, Transporte, População\n",
    "        # Buscando essas informações com base nas seções encontradas\n",
    "\n",
    "        city_information += f\"Informações encontradas sobre:{city_name}\\n\"\n",
    "        \n",
    "        # Buscando seções de interesse\n",
    "        for section in soup.find_all(['h2', 'h3', 'h4']):\n",
    "            section_text = section.get_text(strip=True).lower()\n",
    "    \n",
    "            if any(keyword in section_text for keyword in ['clima', 'economia', 'educação', 'transporte', 'população','lazer','bairros','gastronomia',\n",
    "                                                           'parques e jardins','demografia','turismo','saude','desporto']):\n",
    "                city_information += f\"\\n{section_text.capitalize()}:\\n\"\n",
    "                \n",
    "                # Pegando o conteúdo da seção e exibindo\n",
    "                section_next = section.find_next(['p', 'ul'])\n",
    "                if section_next:\n",
    "                    city_information += f\"- {section_next.get_text(strip=True)}\\n\"\n",
    "        \n",
    " # Exemplo: Extrair tabelas (como infoboxes)\n",
    "        city_information += \"\\nInfobox encontrados:\"\n",
    "        infobox = soup.find('table', {'class': 'infobox'})\n",
    "        if infobox:\n",
    "            # Para cada linha da tabela do infobox\n",
    "            for row in infobox.find_all('tr'):\n",
    "                # Encontrando os <td> de cada linha (dados e valores)\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) == 2:  # A linha tem 2 células (título e valor)\n",
    "                    title = cells[0].get_text(strip=True)\n",
    "                    value = cells[1].get_text(strip=True)\n",
    "\n",
    "                    # Exibe o título e o valor extraído\n",
    "                    city_information += f\"- {title}: {value}\\n\"\n",
    "                    \n",
    "                    # Caso haja links, podemos mostrar o URL\n",
    "                    \n",
    "                    # link = cells[1].find('a')\n",
    "                    # if link:\n",
    "                    #     link_url = link.get('href')\n",
    "                    #     print(f\"  Link: {link_url}\")\n",
    "\n",
    "    else:\n",
    "        city_information += f\"Página '{city_name}' não encontrada.\"\n",
    "\n",
    "    return city_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1996ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso:\n",
    "city_name = 'Setúbal'\n",
    "city_info = get_city_info_wikipedia(city_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70ac905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '..\\\\..\\\\Bases\\\\Wikipedia\\\\' + city_name + '.txt'\n",
    "# save information to a file\n",
    "with open(filepath, 'w', encoding='utf-8') as file:\n",
    "    file.write(city_info)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
